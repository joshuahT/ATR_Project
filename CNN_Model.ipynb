{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d3a31325",
      "metadata": {
        "id": "d3a31325"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import nibabel as nib\n",
        "\n",
        "from scipy import ndimage\n",
        "\n",
        "\n",
        "def read_nifti_file(filepath):\n",
        "    \"\"\"Read and load volume\"\"\"\n",
        "    # Read file\n",
        "    scan = nib.load(filepath)\n",
        "    # Get raw data\n",
        "    scan = scan.get_fdata()\n",
        "    return scan\n",
        "\n",
        "\n",
        "def normalize(volume):\n",
        "    \"\"\"Normalize the volume\"\"\"\n",
        "    min = -1000\n",
        "    max = 400\n",
        "    volume[volume < min] = min\n",
        "    volume[volume > max] = max\n",
        "    volume = (volume - min) / (max - min)\n",
        "    volume = volume.astype(\"float32\")\n",
        "    return volume\n",
        "\n",
        "\n",
        "def resize_volume(img):\n",
        "    \"\"\"Resize across z-axis\"\"\"\n",
        "    # Set the desired depth\n",
        "    desired_depth = 64\n",
        "    desired_width = 128\n",
        "    desired_height = 128\n",
        "    # Get current depth\n",
        "    current_depth = img.shape[-1]\n",
        "    current_width = img.shape[0]\n",
        "    current_height = img.shape[1]\n",
        "    # Compute depth factor\n",
        "    depth = current_depth / desired_depth\n",
        "    width = current_width / desired_width\n",
        "    height = current_height / desired_height\n",
        "    depth_factor = 1 / depth\n",
        "    width_factor = 1 / width\n",
        "    height_factor = 1 / height\n",
        "    # Rotate\n",
        "    img = ndimage.rotate(img, 90, reshape=False)\n",
        "    # Resize across z-axis\n",
        "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
        "    return img\n",
        "\n",
        "\n",
        "def process_scan(path):\n",
        "    \"\"\"Read and resize volume\"\"\"\n",
        "    # Read scan\n",
        "    volume = read_nifti_file(path)\n",
        "    # Normalize\n",
        "    volume = normalize(volume)\n",
        "    # Resize width, height and depth\n",
        "    volume = resize_volume(volume)\n",
        "    return volume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9e1a3ea5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e1a3ea5",
        "outputId": "b9576de1-3cc3-4154-9f3b-dac6d7fd2c30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scans with non target: 707\n",
            "scans with saline: 106\n",
            "scans with rubber: 112\n",
            "scans with clay: 80\n"
          ]
        }
      ],
      "source": [
        "# creating the paths for each respective class folder \n",
        "non_target_paths = [\n",
        "    os.path.join(os.getcwd(), \"labeled_data/data/0\", x)\n",
        "    for x in os.listdir(\"labeled_data/data/0\")\n",
        "]\n",
        "\n",
        "\n",
        "saline_paths = [\n",
        "    os.path.join(os.getcwd(), \"labeled_data/data/1\", x)\n",
        "    for x in os.listdir(\"labeled_data/data/1\")\n",
        "]\n",
        "\n",
        "rubber_paths = [\n",
        "    os.path.join(os.getcwd(), \"labeled_data/data/2\", x)\n",
        "    for x in os.listdir(\"labeled_data/data/2\")\n",
        "]\n",
        "\n",
        "clay_paths = [\n",
        "    os.path.join(os.getcwd(), \"labeled_data/data/3\", x)\n",
        "    for x in os.listdir(\"labeled_data/data/3\")\n",
        "]\n",
        "\n",
        "print(\"scans with non target: \" + str(len(non_target_paths)))\n",
        "print(\"scans with saline: \" + str(len(saline_paths)))\n",
        "print(\"scans with rubber: \" + str(len(rubber_paths)))\n",
        "print(\"scans with clay: \" + str(len(clay_paths)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f86dc969",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f86dc969",
        "outputId": "cddd1de2-037f-477f-c4b3-04c92449fd0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples in train and validation are 1281 and 320.\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils import resample\n",
        "# Read and process the scans.\n",
        "# Each scan is resized across height, width, and depth and rescaled.\n",
        "non_target_scans = np.array([process_scan(path) for path in non_target_paths])\n",
        "saline_scans = np.array([process_scan(path) for path in saline_paths])\n",
        "rubber_scans = np.array([process_scan(path) for path in rubber_paths])\n",
        "clay_scans = np.array([process_scan(path) for path in clay_paths])\n",
        "\n",
        "# max_non = np.max([image.shape for image in non_target_scans], axis=0)\n",
        "# max_saline = np.max([image.shape for image in saline_scans], axis=0)\n",
        "# max_rubber = np.max([image.shape for image in rubber_scans], axis=0)\n",
        "# max_clay = np.max([image.shape for image in clay_scans], axis=0)\n",
        "\n",
        "\n",
        "# print(max_non)\n",
        "# print(max_saline)\n",
        "# print(max_rubber)\n",
        "# print(max_clay)\n",
        "\n",
        "# assign 1 for target, for the non_target ones assign 0.\n",
        "non_target_labels = np.array([0 for _ in range(len(non_target_scans))])\n",
        "saline_labels = np.array([1 for _ in range(len(saline_scans))])\n",
        "rubber_labels = np.array([1 for _ in range(len(rubber_scans))])\n",
        "clay_labels = np.array([1 for _ in range(len(clay_scans))])\n",
        "\n",
        "\n",
        "def generate_rotations(nparray, labels):\n",
        "    for img in nparray:\n",
        "        for i in range(2):\n",
        "            new_img = np.rot90(img)\n",
        "            nparray = np.append(nparray, np.expand_dims(new_img, axis=0), axis=0)\n",
        "            labels = np.append(labels, labels[-1])\n",
        "    return nparray, labels\n",
        "\n",
        "saline_scans, saline_labels = generate_rotations(saline_scans, saline_labels)\n",
        "rubber_scans, rubber_labels = generate_rotations(rubber_scans, rubber_labels)\n",
        "clay_scans, clay_labels = generate_rotations(clay_scans, clay_labels)\n",
        "\n",
        "# print(\"scans with saline: \" + str(len(saline_scans)))\n",
        "# print(\"scans with rubber: \" + str(len(rubber_scans)))\n",
        "# print(\"scans with clay: \" + str(len(clay_scans)))\n",
        "\n",
        "\n",
        "x_train = np.concatenate((non_target_scans[:round(len(non_target_scans)*0.8)], saline_scans[:round(len(saline_scans)*0.8)], rubber_scans[:round(len(rubber_scans)*0.8)], clay_scans[:round(len(clay_scans)*0.8)]), axis=0)\n",
        "y_train = np.concatenate((non_target_labels[:round(len(non_target_labels)*0.8)], saline_labels[:round(len(saline_labels)*0.8)], rubber_labels[:round(len(rubber_labels)*0.8)], clay_labels[:round(len(clay_labels)*0.8)]), axis=0)\n",
        "x_test = np.concatenate((non_target_scans[round(len(non_target_scans)*0.8):], saline_scans[round(len(saline_scans)*0.8):], rubber_scans[round(len(rubber_scans)*0.8):], clay_scans[round(len(clay_scans)*0.8):]), axis=0)\n",
        "y_test = np.concatenate((non_target_labels[round(len(non_target_labels)*0.8):], saline_labels[round(len(saline_labels)*0.8):], rubber_labels[round(len(rubber_labels)*0.8):], clay_labels[round(len(clay_labels)*0.8):]), axis=0)\n",
        "print(\n",
        "    \"Number of samples in train and validation are %d and %d.\"\n",
        "    % (x_train.shape[0], x_test.shape[0])\n",
        ")\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 2)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 2)\n",
        "\n",
        "print(y_train)\n",
        "\n",
        "\n",
        "# # Undersample the non-targets class (WAS NOT ACCURATE DID NOT WORK)\n",
        "\n",
        "# n_minority = min(len(saline_scans), len(rubber_scans), len(clay_scans))\n",
        "# non_target_scans_undersampled, non_target_labels_undersampled = resample(non_target_scans,\n",
        "#                                                                          non_target_labels,\n",
        "#                                                                          replace=False,  # Sample without replacement\n",
        "#                                                                          n_samples=400,  # Number of samples to match minority class\n",
        "#                                                                          )  # Set random seed for reproducibility\n",
        "\n",
        "# # Combine the minority classes and the undersampled non-targets class\n",
        "# x_train = np.concatenate((non_target_scans_undersampled, saline_scans, rubber_scans, clay_scans), axis=0)\n",
        "# y_train = np.concatenate((non_target_labels_undersampled, saline_labels, rubber_labels, clay_labels), axis=0)\n",
        "# x_val = np.concatenate((non_target_scans[round(len(non_target_scans)*0.8):], saline_scans[round(len(saline_scans)*0.8):], rubber_scans[round(len(rubber_scans)*0.8):], clay_scans[round(len(clay_scans)*0.8):]), axis=0)\n",
        "# y_val = np.concatenate((non_target_labels[round(len(non_target_labels)*0.8):], saline_labels[round(len(saline_labels)*0.8):], rubber_labels[round(len(rubber_labels)*0.8):], clay_labels[round(len(clay_labels)*0.8):]), axis=0)\n",
        "# print(\n",
        "#     \"Number of samples in train and validation are %d and %d.\"\n",
        "#     % (x_train.shape[0], x_val.shape[0])\n",
        "# )\n",
        "\n",
        "# Split data in the ratio 80-20 for training and validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "53757271",
      "metadata": {
        "id": "53757271"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "from scipy import ndimage\n",
        "\n",
        "def train_preprocessing(volume, label):\n",
        "    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n",
        "    volume = tf.expand_dims(volume, axis=3)\n",
        "    return volume, label\n",
        "\n",
        "\n",
        "def validation_preprocessing(volume, label):\n",
        "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
        "    volume = tf.expand_dims(volume, axis=3)\n",
        "    return volume, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e20d59bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "e20d59bc",
        "outputId": "7171933d-6c64-4808-cd0d-d6cadc06b5fe"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_x, valid_x, train_y, valid_y = train_test_split(x_train,y_train, test_size=0.2) # remove when done with gridsearch\n",
        "\n",
        "# Define data loaders.\n",
        "train_loader = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
        "validation_loader = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
        "\n",
        "batch_size = 16\n",
        "# Augment the on the fly during training.\n",
        "train_dataset = (\n",
        "    train_loader.shuffle(len(train_x))\n",
        "    .map(train_preprocessing)\n",
        "    .batch(batch_size)\n",
        ")\n",
        "# Only rescale.\n",
        "validation_dataset = (\n",
        "    validation_loader.shuffle(len(valid_x))\n",
        "    .map(validation_preprocessing)\n",
        "    .batch(batch_size)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8df5c80",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# recreation of CNN architecture to perform gridsearchCV to tune hyper parameters \n",
        "\n",
        "# SKIP\n",
        "\n",
        "def create_model(filters, kernel_size, dense_units, dropout):\n",
        "    inputs = keras.Input((128, 128, 64, 1))\n",
        "    x = layers.Conv3D(filters=filters, kernel_size=kernel_size, activation=\"relu\")(inputs)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv3D(filters=filters, kernel_size=kernel_size, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv3D(filters=2*filters, kernel_size=kernel_size, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv3D(filters=4*filters, kernel_size=kernel_size, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling3D()(x)\n",
        "    x = layers.Dense(units=dense_units, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    outputs = layers.Dense(units=2, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Define hyperparameters to search\n",
        "param_grid = {\n",
        "    'filters': [32, 64],\n",
        "    'kernel_size': [3, 5],\n",
        "    'dense_units': [128, 256],\n",
        "    'dropout': [0.3, 0.5]\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Create KerasClassifier for grid search\n",
        "model = KerasClassifier(build_fn=create_model, batch_size=2, verbose=1)\n",
        "\n",
        "# Run grid search with cross-validation\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
        "grid_result = grid.fit(train_x,train_y, validation_data=(valid_x,valid_y))\n",
        "\n",
        "# Print best results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "373c2015",
      "metadata": {
        "id": "373c2015",
        "outputId": "5a0df71f-8c88-4438-f499-090010516c50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"3dcnn\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 128, 128, 64, 1)  0         \n",
            "                             ]                                   \n",
            "                                                                 \n",
            " conv3d_4 (Conv3D)           (None, 126, 126, 62, 64)  1792      \n",
            "                                                                 \n",
            " max_pooling3d_4 (MaxPooling  (None, 63, 63, 31, 64)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 63, 63, 31, 64)   256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv3d_5 (Conv3D)           (None, 61, 61, 29, 64)    110656    \n",
            "                                                                 \n",
            " max_pooling3d_5 (MaxPooling  (None, 30, 30, 14, 64)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 30, 30, 14, 64)   256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv3d_6 (Conv3D)           (None, 28, 28, 12, 128)   221312    \n",
            "                                                                 \n",
            " max_pooling3d_6 (MaxPooling  (None, 14, 14, 6, 128)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 14, 14, 6, 128)   512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv3d_7 (Conv3D)           (None, 12, 12, 4, 256)    884992    \n",
            "                                                                 \n",
            " max_pooling3d_7 (MaxPooling  (None, 6, 6, 2, 256)     0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 6, 6, 2, 256)     1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " global_average_pooling3d_1   (None, 256)              0         \n",
            " (GlobalAveragePooling3D)                                        \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,253,954\n",
            "Trainable params: 1,252,930\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def get_model(width=128, height=128, depth=64):\n",
        "    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
        "\n",
        "    # gridsearchCV, 1 ---------------------------------------------------------------------------------------\n",
        "\n",
        "    # inputs = keras.Input((width, height, depth, 1))\n",
        "\n",
        "\n",
        "    # x = layers.Conv3D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    # x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = layers.Conv3D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "    # x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "    # x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "    # x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = layers.GlobalAveragePooling3D()(x)\n",
        "    # x = layers.Dense(units=256, activation=\"relu\")(x)\n",
        "    # x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    # outputs = layers.Dense(units=2, activation=\"sigmoid\")(x)\n",
        "\n",
        "\n",
        "    # # Define the model.\n",
        "    # model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
        "    # return model\n",
        "\n",
        "    #gridsearchCV, {'dense_units': 128, 'dropout': 0.5, 'filters': 64, 'kernel_size': 3} , removed the rotation from the preprocessing pipeline\n",
        "    inputs = keras.Input((width, height, depth, 1))\n",
        "\n",
        "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling3D()(x)\n",
        "    x = layers.Dense(units=128, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    outputs = layers.Dense(units=2, activation=\"sigmoid\")(x)\n",
        "\n",
        "\n",
        "    # Define the model.\n",
        "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
        "    return model\n",
        "\n",
        "\n",
        "    #inital architecture no tuning ------------------------------------------------------------------------------------\n",
        "\n",
        "    # inputs = keras.Input((width, height, depth, 1))\n",
        "\n",
        "    # x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    # x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "    # x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "    # x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "    # x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = layers.GlobalAveragePooling3D()(x)\n",
        "    # x = layers.Dense(units=512, activation=\"relu\")(x)\n",
        "    # x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    # outputs = layers.Dense(units=2, activation=\"sigmoid\")(x)\n",
        "\n",
        "    # # Define the model.\n",
        "    # model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
        "    # return model\n",
        "\n",
        "\n",
        "model = get_model(width=128, height=128, depth=64)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5ee7330b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(gpus)\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "88ca0c26",
      "metadata": {
        "id": "88ca0c26",
        "outputId": "fa1e7c4f-85b7-4d74-e4e6-89171f15eac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            " 3/64 [>.............................] - ETA: 13:29 - loss: 0.6375 - acc: 0.6042"
          ]
        }
      ],
      "source": [
        "# print(d_class_weights)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Compile model.\n",
        "initial_learning_rate = 0.0001\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
        ")\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "    metrics=[\"acc\"],\n",
        ")\n",
        "\n",
        "# Define callbacks.\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
        "    \"3d_image_classification.h5\", save_best_only=True\n",
        ")\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
        "\n",
        "# Train the model, doing validation at the end of each epoch\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=100,\n",
        "    shuffle=True,\n",
        "    verbose=1,\n",
        "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
        "    # class_weight = d_class_weights\n",
        ")\n",
        "\n",
        "\n",
        "# Plot accuracy vs validation accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#  loss: 0.9537 - acc: 0.7065 - val_loss: 0.9253 - val_acc: 0.6894"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24b25788",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model.load_weights(\"3d_image_classification.h5\")\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "acc = accuracy_score(y_test_labels, y_pred_labels)\n",
        "print(\"Accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eff39b2",
      "metadata": {
        "id": "9eff39b2",
        "outputId": "31e72b05-39de-4b3a-9236-935434e6a13a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "unlabeled_data_paths = [\n",
        "    os.path.join(os.getcwd(), \"unlabeled_data/\", x)\n",
        "    for x in os.listdir(\"unlabeled_data/\")\n",
        "]\n",
        "\n",
        "print(\"scans with unlabeled data: \" + str(len(unlabeled_data_paths)))\n",
        "\n",
        "unlabeled_data_scans = np.array([process_scan(path) for path in unlabeled_data_paths])\n",
        "\n",
        "\n",
        "gt_y = model.predict(unlabeled_data_scans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0bf679b",
      "metadata": {
        "id": "f0bf679b"
      },
      "outputs": [],
      "source": [
        "print(gt_y.shape)\n",
        "print(type(gt_y))\n",
        "class_labels = [\"0\", \"1\"] # replace with your own class labels\n",
        "predicted_labels = [class_labels[prediction.argmax()] for prediction in gt_y]\n",
        "\n",
        "print(predicted_labels)\n",
        "\n",
        "class_counts = np.bincount(predicted_labels) # count the number of occurrences of each predicted class\n",
        "print(class_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f5b1636",
      "metadata": {},
      "outputs": [],
      "source": [
        "ground_truth = pd.read_excel('testing_filenames.xlsx', names=['filename'], index_col=None, header=None)\n",
        "for i in ground_truth.index:\n",
        "    filename = ground_truth['filename'].iloc[i]\n",
        "\n",
        "df = pd.DataFrame({'filename': ground_truth['filename'], 'label': predicted_labels})\n",
        "df = df.set_index('filename')\n",
        "df = df.loc[ground_truth['filename']]\n",
        "df.to_excel('y_predictions_new1.xlsx')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
