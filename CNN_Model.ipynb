{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d3a31325",
      "metadata": {
        "id": "d3a31325"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import nibabel as nib\n",
        "\n",
        "from scipy import ndimage\n",
        "\n",
        "\n",
        "def read_nifti_file(filepath):\n",
        "    \"\"\"Read and load volume\"\"\"\n",
        "    # Read file\n",
        "    scan = nib.load(filepath)\n",
        "    # Get raw data\n",
        "    scan = scan.get_fdata()\n",
        "    return scan\n",
        "\n",
        "\n",
        "def normalize(volume):\n",
        "    \"\"\"Normalize the volume\"\"\"\n",
        "    min = -1000\n",
        "    max = 400\n",
        "    volume[volume < min] = min\n",
        "    volume[volume > max] = max\n",
        "    volume = (volume - min) / (max - min)\n",
        "    volume = volume.astype(\"float32\")\n",
        "    return volume\n",
        "\n",
        "\n",
        "def resize_volume(img):\n",
        "    \"\"\"Resize across z-axis\"\"\"\n",
        "    # Set the desired depth\n",
        "    desired_depth = 64\n",
        "    desired_width = 128\n",
        "    desired_height = 128\n",
        "    # Get current depth\n",
        "    current_depth = img.shape[-1]\n",
        "    current_width = img.shape[0]\n",
        "    current_height = img.shape[1]\n",
        "    # Compute depth factor\n",
        "    depth = current_depth / desired_depth\n",
        "    width = current_width / desired_width\n",
        "    height = current_height / desired_height\n",
        "    depth_factor = 1 / depth\n",
        "    width_factor = 1 / width\n",
        "    height_factor = 1 / height\n",
        "    # Rotate\n",
        "    img = ndimage.rotate(img, 90, reshape=False)\n",
        "    # Resize across z-axis\n",
        "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
        "    return img\n",
        "\n",
        "\n",
        "def process_scan(path):\n",
        "    \"\"\"Read and resize volume\"\"\"\n",
        "    # Read scan\n",
        "    volume = read_nifti_file(path)\n",
        "    # Normalize\n",
        "    volume = normalize(volume)\n",
        "    # Resize width, height and depth\n",
        "    volume = resize_volume(volume)\n",
        "    return volume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v6oHQqyZVCWR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6oHQqyZVCWR",
        "outputId": "3b0b8957-fe9e-4440-ad94-94848f91db71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'ATR_Project'\n",
            "/content/ATR_Project\n"
          ]
        }
      ],
      "source": [
        "%cd ATR_Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9e1a3ea5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e1a3ea5",
        "outputId": "b9576de1-3cc3-4154-9f3b-dac6d7fd2c30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scans with non target: 707\n",
            "scans with saline: 106\n",
            "scans with rubber: 112\n",
            "scans with clay: 80\n"
          ]
        }
      ],
      "source": [
        "# Folder \"CT-0\" consist of CT scans having normal lung tissue,\n",
        "# no CT-signs of viral pneumonia.\n",
        "non_target_paths = [\n",
        "    os.path.join(os.getcwd(), \"labeled_data/data/0\", x)\n",
        "    for x in os.listdir(\"labeled_data/data/0\")\n",
        "]\n",
        "\n",
        "\n",
        "saline_paths = [\n",
        "    os.path.join(os.getcwd(), \"labeled_data/data/1\", x)\n",
        "    for x in os.listdir(\"labeled_data/data/1\")\n",
        "]\n",
        "\n",
        "rubber_paths = [\n",
        "    os.path.join(os.getcwd(), \"labeled_data/data/2\", x)\n",
        "    for x in os.listdir(\"labeled_data/data/2\")\n",
        "]\n",
        "\n",
        "clay_paths = [\n",
        "    os.path.join(os.getcwd(), \"labeled_data/data/3\", x)\n",
        "    for x in os.listdir(\"labeled_data/data/3\")\n",
        "]\n",
        "\n",
        "print(\"scans with non target: \" + str(len(non_target_paths)))\n",
        "print(\"scans with saline: \" + str(len(saline_paths)))\n",
        "print(\"scans with rubber: \" + str(len(rubber_paths)))\n",
        "print(\"scans with clay: \" + str(len(clay_paths)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f86dc969",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f86dc969",
        "outputId": "cddd1de2-037f-477f-c4b3-04c92449fd0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples in train and validation are 725 and 280.\n"
          ]
        }
      ],
      "source": [
        "# Read and process the scans.\n",
        "# Each scan is resized across height, width, and depth and rescaled.\n",
        "non_target_scans = np.array([process_scan(path) for path in non_target_paths])\n",
        "saline_scans = np.array([process_scan(path) for path in saline_paths])\n",
        "rubber_scans = np.array([process_scan(path) for path in rubber_paths])\n",
        "clay_scans = np.array([process_scan(path) for path in clay_paths])\n",
        "\n",
        "# For the CT scans having presence of viral pneumonia\n",
        "# assign 1, for the normal ones assign 0.\n",
        "non_target_labels = np.array([0 for _ in range(len(non_target_scans))])\n",
        "saline_labels = np.array([1 for _ in range(len(saline_scans))])\n",
        "rubber_labels = np.array([2 for _ in range(len(rubber_scans))])\n",
        "clay_labels = np.array([3 for _ in range(len(clay_scans))])\n",
        "\n",
        "# Split data in the ratio 70-30 for training and validation.\n",
        "x_train = np.concatenate((non_target_scans[70:], saline_scans[70:],rubber_scans[70:], clay_scans[70:]), axis=0)\n",
        "y_train = np.concatenate((non_target_labels[70:], saline_labels[70:], rubber_labels[70:], clay_labels[70:]), axis=0)\n",
        "x_val = np.concatenate((non_target_scans[:70], saline_scans[:70],rubber_scans[:70], clay_scans[:70]), axis=0)\n",
        "y_val = np.concatenate((non_target_labels[:70], saline_labels[:70], rubber_labels[:70], clay_labels[:70]), axis=0)\n",
        "print(\n",
        "    \"Number of samples in train and validation are %d and %d.\"\n",
        "    % (x_train.shape[0], x_val.shape[0])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "53757271",
      "metadata": {
        "id": "53757271"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "from scipy import ndimage\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def rotate(volume):\n",
        "    \"\"\"Rotate the volume by a few degrees\"\"\"\n",
        "\n",
        "    def scipy_rotate(volume):\n",
        "        # define some rotation angles\n",
        "        angles = [-20, -10, -5, 5, 10, 20]\n",
        "        # pick angles at random\n",
        "        angle = random.choice(angles)\n",
        "        # rotate volume\n",
        "        volume = ndimage.rotate(volume, angle, reshape=False)\n",
        "        volume[volume < 0] = 0\n",
        "        volume[volume > 1] = 1\n",
        "        return volume\n",
        "\n",
        "    augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n",
        "    return augmented_volume\n",
        "\n",
        "\n",
        "def train_preprocessing(volume, label):\n",
        "    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n",
        "    # Rotate volume\n",
        "    volume = rotate(volume)\n",
        "    volume = tf.expand_dims(volume, axis=3)\n",
        "    return volume, label\n",
        "\n",
        "\n",
        "def validation_preprocessing(volume, label):\n",
        "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
        "    volume = tf.expand_dims(volume, axis=3)\n",
        "    return volume, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e20d59bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "e20d59bc",
        "outputId": "7171933d-6c64-4808-cd0d-d6cadc06b5fe"
      },
      "outputs": [],
      "source": [
        "# Define data loaders.\n",
        "train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "validation_loader = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "\n",
        "batch_size = 2\n",
        "# Augment the on the fly during training.\n",
        "train_dataset = (\n",
        "    train_loader.shuffle(len(x_train))\n",
        "    .map(train_preprocessing)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(2)\n",
        ")\n",
        "# Only rescale.\n",
        "validation_dataset = (\n",
        "    validation_loader.shuffle(len(x_val))\n",
        "    .map(validation_preprocessing)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(2)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "373c2015",
      "metadata": {
        "id": "373c2015",
        "outputId": "5a0df71f-8c88-4438-f499-090010516c50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"3dcnn\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 128, 128, 64, 3)  0         \n",
            "                             ]                                   \n",
            "                                                                 \n",
            " conv3d (Conv3D)             (None, 126, 126, 62, 64)  5248      \n",
            "                                                                 \n",
            " max_pooling3d (MaxPooling3D  (None, 63, 63, 31, 64)   0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 63, 63, 31, 64)   256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv3d_1 (Conv3D)           (None, 61, 61, 29, 64)    110656    \n",
            "                                                                 \n",
            " max_pooling3d_1 (MaxPooling  (None, 30, 30, 14, 64)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 30, 30, 14, 64)   256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv3d_2 (Conv3D)           (None, 28, 28, 12, 128)   221312    \n",
            "                                                                 \n",
            " max_pooling3d_2 (MaxPooling  (None, 14, 14, 6, 128)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 14, 14, 6, 128)   512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv3d_3 (Conv3D)           (None, 12, 12, 4, 256)    884992    \n",
            "                                                                 \n",
            " max_pooling3d_3 (MaxPooling  (None, 6, 6, 2, 256)     0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 6, 6, 2, 256)     1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " global_average_pooling3d (G  (None, 256)              0         \n",
            " lobalAveragePooling3D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               131584    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 2052      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,357,892\n",
            "Trainable params: 1,356,868\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def get_model(width=128, height=128, depth=64):\n",
        "    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
        "\n",
        "    inputs = keras.Input((width, height, depth, 3))\n",
        "\n",
        "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling3D()(x)\n",
        "    x = layers.Dense(units=512, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    outputs = layers.Dense(units=4, activation=\"sigmoid\")(x)\n",
        "\n",
        "    # Define the model.\n",
        "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# Build model.\n",
        "model = get_model(width=128, height=128, depth=64)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "88ca0c26",
      "metadata": {
        "id": "88ca0c26",
        "outputId": "fa1e7c4f-85b7-4d74-e4e6-89171f15eac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\losses.py\", line 2162, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend.py\", line 5677, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 4) vs (None, 1)).\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39m# Train the model, doing validation at the end of each epoch\u001b[39;00m\n\u001b[0;32m     19\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m---> 20\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     21\u001b[0m     train_dataset,\n\u001b[0;32m     22\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_dataset,\n\u001b[0;32m     23\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     24\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     25\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[0;32m     26\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[checkpoint_cb, early_stopping_cb],\n\u001b[0;32m     27\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mC:\\Users\\JOSHUA~1\\AppData\\Local\\Temp\\__autograph_generated_file2vv7pjz1.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\losses.py\", line 2162, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"c:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend.py\", line 5677, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 4) vs (None, 1)).\n"
          ]
        }
      ],
      "source": [
        "# Compile model.\n",
        "initial_learning_rate = 0.0001\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
        ")\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "    metrics=[\"acc\"],\n",
        ")\n",
        "\n",
        "# Define callbacks.\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
        "    \"3d_image_classification.h5\", save_best_only=True\n",
        ")\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
        "\n",
        "# Train the model, doing validation at the end of each epoch\n",
        "epochs = 10\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=epochs,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eff39b2",
      "metadata": {
        "id": "9eff39b2",
        "outputId": "31e72b05-39de-4b3a-9236-935434e6a13a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimension of the CT scan is: (128, 128, 64, 1)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x22c16561180>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = train_dataset.take(1)\n",
        "images, labels = list(data)[0]\n",
        "images = images.numpy()\n",
        "image = images[0]\n",
        "print(\"Dimension of the CT scan is:\", image.shape)\n",
        "plt.imshow(np.squeeze(image[:, :, 30]), cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0bf679b",
      "metadata": {
        "id": "f0bf679b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
