{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d3a31325",
      "metadata": {
        "id": "d3a31325"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import nibabel as nib\n",
        "\n",
        "from scipy import ndimage\n",
        "\n",
        "\n",
        "def read_nifti_file(filepath):\n",
        "    \"\"\"Read and load volume\"\"\"\n",
        "    # Read file\n",
        "    scan = nib.load(filepath)\n",
        "    # Get raw data\n",
        "    scan = scan.get_fdata()\n",
        "    return scan\n",
        "\n",
        "\n",
        "def normalize(volume):\n",
        "    \"\"\"Normalize the volume\"\"\"\n",
        "    min = -1000\n",
        "    max = 400\n",
        "    volume[volume < min] = min\n",
        "    volume[volume > max] = max\n",
        "    volume = (volume - min) / (max - min)\n",
        "    volume = volume.astype(\"float32\")\n",
        "    return volume\n",
        "\n",
        "\n",
        "def resize_volume(img):\n",
        "    \"\"\"Resize across z-axis\"\"\"\n",
        "    # Set the desired depth\n",
        "    desired_depth = 64\n",
        "    desired_width = 128\n",
        "    desired_height = 128\n",
        "    # Get current depth\n",
        "    current_depth = img.shape[-1]\n",
        "    current_width = img.shape[0]\n",
        "    current_height = img.shape[1]\n",
        "    # Compute depth factor\n",
        "    depth = current_depth / desired_depth\n",
        "    width = current_width / desired_width\n",
        "    height = current_height / desired_height\n",
        "    depth_factor = 1 / depth\n",
        "    width_factor = 1 / width\n",
        "    height_factor = 1 / height\n",
        "    # Rotate\n",
        "    img = ndimage.rotate(img, 90, reshape=False)\n",
        "    # Resize across z-axis\n",
        "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
        "    return img\n",
        "\n",
        "\n",
        "def process_scan(path):\n",
        "    \"\"\"Read and resize volume\"\"\"\n",
        "    # Read scan\n",
        "    volume = read_nifti_file(path)\n",
        "    # Normalize\n",
        "    volume = normalize(volume)\n",
        "    # Resize width, height and depth\n",
        "    volume = resize_volume(volume)\n",
        "    return volume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9e1a3ea5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e1a3ea5",
        "outputId": "b9576de1-3cc3-4154-9f3b-dac6d7fd2c30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scans with non target: 707\n",
            "scans with saline: 106\n",
            "scans with rubber: 112\n",
            "scans with clay: 80\n"
          ]
        }
      ],
      "source": [
        "# creating the paths for each respective class folder \n",
        "non_target_paths = [\n",
        "    os.path.join(os.getcwd(), \"labeled_data/data/0\", x)\n",
        "    for x in os.listdir(\"labeled_data/data/0\")\n",
        "]\n",
        "\n",
        "\n",
        "saline_paths = [\n",
        "    os.path.join(os.getcwd(), \"labeled_data/data/1\", x)\n",
        "    for x in os.listdir(\"labeled_data/data/1\")\n",
        "]\n",
        "\n",
        "rubber_paths = [\n",
        "    os.path.join(os.getcwd(), \"labeled_data/data/2\", x)\n",
        "    for x in os.listdir(\"labeled_data/data/2\")\n",
        "]\n",
        "\n",
        "clay_paths = [\n",
        "    os.path.join(os.getcwd(), \"labeled_data/data/3\", x)\n",
        "    for x in os.listdir(\"labeled_data/data/3\")\n",
        "]\n",
        "\n",
        "print(\"scans with non target: \" + str(len(non_target_paths)))\n",
        "print(\"scans with saline: \" + str(len(saline_paths)))\n",
        "print(\"scans with rubber: \" + str(len(rubber_paths)))\n",
        "print(\"scans with clay: \" + str(len(clay_paths)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f86dc969",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f86dc969",
        "outputId": "cddd1de2-037f-477f-c4b3-04c92449fd0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples in train and validation are 378 and 80.\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils import resample\n",
        "# Read and process the scans.\n",
        "# Each scan is resized across height, width, and depth and rescaled.\n",
        "non_target_scans = np.array([process_scan(path) for path in non_target_paths])\n",
        "saline_scans = np.array([process_scan(path) for path in saline_paths])\n",
        "rubber_scans = np.array([process_scan(path) for path in rubber_paths])\n",
        "clay_scans = np.array([process_scan(path) for path in clay_paths])\n",
        "\n",
        "# max_non = np.max([image.shape for image in non_target_scans], axis=0)\n",
        "# max_saline = np.max([image.shape for image in saline_scans], axis=0)\n",
        "# max_rubber = np.max([image.shape for image in rubber_scans], axis=0)\n",
        "# max_clay = np.max([image.shape for image in clay_scans], axis=0)\n",
        "\n",
        "\n",
        "# print(max_non)\n",
        "# print(max_saline)\n",
        "# print(max_rubber)\n",
        "# print(max_clay)\n",
        "\n",
        "# assign 1 for target, for the non_target ones assign 0.\n",
        "non_target_labels = np.array([0 for _ in range(len(non_target_scans))])\n",
        "saline_labels = np.array([1 for _ in range(len(saline_scans))])\n",
        "rubber_labels = np.array([1 for _ in range(len(rubber_scans))])\n",
        "clay_labels = np.array([1 for _ in range(len(clay_scans))])\n",
        "\n",
        "\n",
        "def generate_rotations(nparray, labels):\n",
        "    for img in nparray:\n",
        "        for i in range(2):\n",
        "            new_img = np.rot90(img)\n",
        "            nparray = np.append(nparray, np.expand_dims(new_img, axis=0), axis=0)\n",
        "            labels = np.append(labels, labels[-1])\n",
        "    return nparray, labels\n",
        "\n",
        "saline_scans, saline_labels = generate_rotations(saline_scans, saline_labels)\n",
        "rubber_scans, rubber_labels = generate_rotations(rubber_scans, rubber_labels)\n",
        "clay_scans, clay_labels = generate_rotations(clay_scans, clay_labels)\n",
        "\n",
        "# print(\"scans with saline: \" + str(len(saline_scans)))\n",
        "# print(\"scans with rubber: \" + str(len(rubber_scans)))\n",
        "# print(\"scans with clay: \" + str(len(clay_scans)))\n",
        "\n",
        "\n",
        "x_train = np.concatenate((non_target_scans[:round(len(non_target_scans)*0.8)], saline_scans[:round(len(saline_scans)*0.8)], rubber_scans[:round(len(rubber_scans)*0.8)], clay_scans[:round(len(clay_scans)*0.8)]), axis=0)\n",
        "y_train = np.concatenate((non_target_labels[:round(len(non_target_labels)*0.8)], saline_labels[:round(len(saline_labels)*0.8)], rubber_labels[:round(len(rubber_labels)*0.8)], clay_labels[:round(len(clay_labels)*0.8)]), axis=0)\n",
        "x_test = np.concatenate((non_target_scans[round(len(non_target_scans)*0.8):], saline_scans[round(len(saline_scans)*0.8):], rubber_scans[round(len(rubber_scans)*0.8):], clay_scans[round(len(clay_scans)*0.8):]), axis=0)\n",
        "y_test = np.concatenate((non_target_labels[round(len(non_target_labels)*0.8):], saline_labels[round(len(saline_labels)*0.8):], rubber_labels[round(len(rubber_labels)*0.8):], clay_labels[round(len(clay_labels)*0.8):]), axis=0)\n",
        "print(\n",
        "    \"Number of samples in train and validation are %d and %d.\"\n",
        "    % (x_train.shape[0], x_test.shape[0])\n",
        ")\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 2)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 2)\n",
        "\n",
        "print(y_train)\n",
        "\n",
        "\n",
        "# # Undersample the non-targets class (WAS NOT ACCURATE DID NOT WORK)\n",
        "\n",
        "# n_minority = min(len(saline_scans), len(rubber_scans), len(clay_scans))\n",
        "# non_target_scans_undersampled, non_target_labels_undersampled = resample(non_target_scans,\n",
        "#                                                                          non_target_labels,\n",
        "#                                                                          replace=False,  # Sample without replacement\n",
        "#                                                                          n_samples=400,  # Number of samples to match minority class\n",
        "#                                                                          )  # Set random seed for reproducibility\n",
        "\n",
        "# # Combine the minority classes and the undersampled non-targets class\n",
        "# x_train = np.concatenate((non_target_scans_undersampled, saline_scans, rubber_scans, clay_scans), axis=0)\n",
        "# y_train = np.concatenate((non_target_labels_undersampled, saline_labels, rubber_labels, clay_labels), axis=0)\n",
        "# x_val = np.concatenate((non_target_scans[round(len(non_target_scans)*0.8):], saline_scans[round(len(saline_scans)*0.8):], rubber_scans[round(len(rubber_scans)*0.8):], clay_scans[round(len(clay_scans)*0.8):]), axis=0)\n",
        "# y_val = np.concatenate((non_target_labels[round(len(non_target_labels)*0.8):], saline_labels[round(len(saline_labels)*0.8):], rubber_labels[round(len(rubber_labels)*0.8):], clay_labels[round(len(clay_labels)*0.8):]), axis=0)\n",
        "# print(\n",
        "#     \"Number of samples in train and validation are %d and %d.\"\n",
        "#     % (x_train.shape[0], x_val.shape[0])\n",
        "# )\n",
        "\n",
        "# Split data in the ratio 80-20 for training and validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "53757271",
      "metadata": {
        "id": "53757271"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "from scipy import ndimage\n",
        "\n",
        "def train_preprocessing(volume, label):\n",
        "    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n",
        "    volume = tf.expand_dims(volume, axis=3)\n",
        "    return volume, label\n",
        "\n",
        "\n",
        "def validation_preprocessing(volume, label):\n",
        "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
        "    volume = tf.expand_dims(volume, axis=3)\n",
        "    return volume, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e20d59bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "e20d59bc",
        "outputId": "7171933d-6c64-4808-cd0d-d6cadc06b5fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10, 128, 128, 64, 1)\n",
            "(10, 4)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_x, valid_x, train_y, valid_y = train_test_split(x_train,y_train, test_size=0.2) # remove when done with gridsearch\n",
        "\n",
        "# Define data loaders.\n",
        "train_loader = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
        "validation_loader = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
        "\n",
        "batch_size = 16\n",
        "# Augment the on the fly during training.\n",
        "train_dataset = (\n",
        "    train_loader.shuffle(len(train_x))\n",
        "    .map(train_preprocessing)\n",
        "    .batch(batch_size)\n",
        ")\n",
        "# Only rescale.\n",
        "validation_dataset = (\n",
        "    validation_loader.shuffle(len(valid_x))\n",
        "    .map(validation_preprocessing)\n",
        "    .batch(batch_size)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8df5c80",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# recreation of CNN architecture to perform gridsearchCV to tune hyper parameters \n",
        "\n",
        "# SKIP\n",
        "\n",
        "def create_model(filters, kernel_size, dense_units, dropout):\n",
        "    inputs = keras.Input((128, 128, 64, 1))\n",
        "    x = layers.Conv3D(filters=filters, kernel_size=kernel_size, activation=\"relu\")(inputs)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv3D(filters=filters, kernel_size=kernel_size, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv3D(filters=2*filters, kernel_size=kernel_size, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv3D(filters=4*filters, kernel_size=kernel_size, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling3D()(x)\n",
        "    x = layers.Dense(units=dense_units, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    outputs = layers.Dense(units=2, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Define hyperparameters to search\n",
        "param_grid = {\n",
        "    'filters': [32, 64],\n",
        "    'kernel_size': [3, 5],\n",
        "    'dense_units': [128, 256],\n",
        "    'dropout': [0.3, 0.5]\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Create KerasClassifier for grid search\n",
        "model = KerasClassifier(build_fn=create_model, batch_size=2, verbose=1)\n",
        "\n",
        "# Run grid search with cross-validation\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
        "grid_result = grid.fit(train_x,train_y, validation_data=(valid_x,valid_y))\n",
        "\n",
        "# Print best results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "373c2015",
      "metadata": {
        "id": "373c2015",
        "outputId": "5a0df71f-8c88-4438-f499-090010516c50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"3dcnn\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 128, 128, 64, 1)  0         \n",
            "                             ]                                   \n",
            "                                                                 \n",
            " conv3d (Conv3D)             (None, 126, 126, 62, 64)  1792      \n",
            "                                                                 \n",
            " max_pooling3d (MaxPooling3D  (None, 63, 63, 31, 64)   0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 63, 63, 31, 64)   256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv3d_1 (Conv3D)           (None, 61, 61, 29, 64)    110656    \n",
            "                                                                 \n",
            " max_pooling3d_1 (MaxPooling  (None, 30, 30, 14, 64)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 30, 30, 14, 64)   256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv3d_2 (Conv3D)           (None, 28, 28, 12, 128)   221312    \n",
            "                                                                 \n",
            " max_pooling3d_2 (MaxPooling  (None, 14, 14, 6, 128)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 14, 14, 6, 128)   512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv3d_3 (Conv3D)           (None, 12, 12, 4, 256)    884992    \n",
            "                                                                 \n",
            " max_pooling3d_3 (MaxPooling  (None, 6, 6, 2, 256)     0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 6, 6, 2, 256)     1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " global_average_pooling3d (G  (None, 256)              0         \n",
            " lobalAveragePooling3D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               131584    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 2052      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,354,436\n",
            "Trainable params: 1,353,412\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def get_model(width=128, height=128, depth=64):\n",
        "    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
        "\n",
        "    # gridsearchCV, 1 ---------------------------------------------------------------------------------------\n",
        "\n",
        "    # inputs = keras.Input((width, height, depth, 1))\n",
        "\n",
        "\n",
        "    # x = layers.Conv3D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    # x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = layers.Conv3D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "    # x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "    # x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "    # x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = layers.GlobalAveragePooling3D()(x)\n",
        "    # x = layers.Dense(units=256, activation=\"relu\")(x)\n",
        "    # x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    # outputs = layers.Dense(units=2, activation=\"sigmoid\")(x)\n",
        "\n",
        "\n",
        "    # # Define the model.\n",
        "    # model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
        "    # return model\n",
        "\n",
        "    #gridsearchCV, {'dense_units': 128, 'dropout': 0.5, 'filters': 64, 'kernel_size': 3} , removed the rotation from the preprocessing pipeline\n",
        "    # inputs = keras.Input((width, height, depth, 1))\n",
        "\n",
        "    # x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    # x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "    # x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "    # x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "    # x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # x = layers.GlobalAveragePooling3D()(x)\n",
        "    # x = layers.Dense(units=128, activation=\"relu\")(x)\n",
        "    # x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    # outputs = layers.Dense(units=2, activation=\"sigmoid\")(x)\n",
        "\n",
        "\n",
        "    # # Define the model.\n",
        "    # model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
        "    # return model\n",
        "\n",
        "\n",
        "    #inital architecture no tuning ------------------------------------------------------------------------------------\n",
        "\n",
        "    inputs = keras.Input((width, height, depth, 1))\n",
        "\n",
        "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling3D()(x)\n",
        "    x = layers.Dense(units=512, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    outputs = layers.Dense(units=2, activation=\"sigmoid\")(x)\n",
        "\n",
        "    # Define the model.\n",
        "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
        "    return model\n",
        "\n",
        "\n",
        "model = get_model(width=128, height=128, depth=64)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5ee7330b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(gpus)\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "88ca0c26",
      "metadata": {
        "id": "88ca0c26",
        "outputId": "fa1e7c4f-85b7-4d74-e4e6-89171f15eac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "38/38 [==============================] - 393s 10s/step - loss: 1.2001 - acc: 0.4974 - val_loss: 1.4044 - val_acc: 0.2500\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m# Train the model, doing validation at the end of each epoch\u001b[39;00m\n\u001b[0;32m     24\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m---> 25\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     26\u001b[0m     train_dataset,\n\u001b[0;32m     27\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_dataset,\n\u001b[0;32m     28\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     29\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     30\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     31\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[checkpoint_cb, early_stopping_cb],\n\u001b[0;32m     32\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[1;32mc:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[1;32mc:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[1;32mc:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[1;32mc:\\Users\\Joshuah Tang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# print(d_class_weights)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Compile model.\n",
        "initial_learning_rate = 0.0001\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
        ")\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "    metrics=[\"acc\"],\n",
        ")\n",
        "\n",
        "# Define callbacks.\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
        "    \"3d_image_classification.h5\", save_best_only=True\n",
        ")\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
        "\n",
        "# Train the model, doing validation at the end of each epoch\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=100,\n",
        "    shuffle=True,\n",
        "    verbose=1,\n",
        "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
        "    # class_weight = d_class_weights\n",
        ")\n",
        "\n",
        "\n",
        "# Plot accuracy vs validation accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#  loss: 0.9537 - acc: 0.7065 - val_loss: 0.9253 - val_acc: 0.6894"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24b25788",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model.load_weights(\"3d_image_classification.h5\")\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "acc = accuracy_score(y_test_labels, y_pred_labels)\n",
        "print(\"Accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eff39b2",
      "metadata": {
        "id": "9eff39b2",
        "outputId": "31e72b05-39de-4b3a-9236-935434e6a13a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "unlabeled_data_paths = [\n",
        "    os.path.join(os.getcwd(), \"unlabeled_data/\", x)\n",
        "    for x in os.listdir(\"unlabeled_data/\")\n",
        "]\n",
        "\n",
        "print(\"scans with unlabeled data: \" + str(len(unlabeled_data_paths)))\n",
        "\n",
        "unlabeled_data_scans = np.array([process_scan(path) for path in unlabeled_data_paths])\n",
        "\n",
        "\n",
        "gt_y = model.predict(unlabeled_data_scans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0bf679b",
      "metadata": {
        "id": "f0bf679b"
      },
      "outputs": [],
      "source": [
        "print(gt_y.shape)\n",
        "print(type(gt_y))\n",
        "class_labels = [\"0\", \"1\"] # replace with your own class labels\n",
        "predicted_labels = [class_labels[prediction.argmax()] for prediction in gt_y]\n",
        "\n",
        "print(predicted_labels)\n",
        "\n",
        "class_counts = np.bincount(predicted_labels) # count the number of occurrences of each predicted class\n",
        "print(class_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f5b1636",
      "metadata": {},
      "outputs": [],
      "source": [
        "ground_truth = pd.read_excel('testing_filenames.xlsx', names=['filename'], index_col=None, header=None)\n",
        "for i in ground_truth.index:\n",
        "    filename = ground_truth['filename'].iloc[i]\n",
        "\n",
        "df = pd.DataFrame({'filename': ground_truth['filename'], 'label': predicted_labels})\n",
        "df = df.set_index('filename')\n",
        "df = df.loc[ground_truth['filename']]\n",
        "df.to_excel('y_predictions_new1.xlsx')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
